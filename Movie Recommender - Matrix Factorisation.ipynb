{
 "cells": [
  {
   "source": [
    "## Algorithme de SVD "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def emsvd(Y, k=None, tol=1E-3, maxiter=5):\n",
    "    \"\"\"\n",
    "    Approximate SVD on data with missing values via expectation-maximization\n",
    "\n",
    "    Inputs:\n",
    "    -----------\n",
    "    Y:          (nobs, ndim) data matrix, missing values denoted by NaN/Inf\n",
    "    k:          number of singular values/vectors to find (default: k=ndim)\n",
    "    tol:        convergence tolerance on change in trace norm\n",
    "    maxiter:    maximum number of EM steps to perform (default: no limit)\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    Y_hat:      (nobs, ndim) reconstructed data matrix\n",
    "    mu_hat:     (ndim,) estimated column means for reconstructed data\n",
    "    U, s, Vt:   singular values and vectors (see np.linalg.svd and \n",
    "                scipy.sparse.linalg.svds for details)\n",
    "    \"\"\"\n",
    "\n",
    "    if k is None:\n",
    "        svdmethod = partial(np.linalg.svd, full_matrices=False)\n",
    "    else:\n",
    "        svdmethod = partial(svds, k=k)\n",
    "    if maxiter is None:\n",
    "        maxiter = np.inf\n",
    "\n",
    "    # initialize the missing values to their respective column means\n",
    "    mu_hat = np.nanmean(Y, axis=0, keepdims=1)\n",
    "    valid = np.isfinite(Y)\n",
    "    Y_hat = np.where(valid, Y, mu_hat)\n",
    "\n",
    "    halt = False\n",
    "    ii = 1\n",
    "    v_prev = 0\n",
    "\n",
    "    while not halt:\n",
    "        \n",
    "    # SVD on filled-in data\n",
    "        U, s, Vt = svdmethod(Y_hat - mu_hat)\n",
    "\n",
    "        # impute missing values\n",
    "        Y_hat[~valid] = (U.dot(np.diag(s)).dot(Vt) + mu_hat)[~valid]\n",
    "\n",
    "        # update bias parameter\n",
    "        mu_hat = Y_hat.mean(axis=0, keepdims=1)\n",
    "\n",
    "        # test convergence using relative change in trace norm\n",
    "        v = s.sum()\n",
    "        if v_prev==0:\n",
    "            flag=1\n",
    "        else:\n",
    "            flag=v_prev\n",
    "        if ii >= maxiter or ((v - v_prev) / flag) < tol:\n",
    "            halt = True\n",
    "        ii += 1\n",
    "        #print(ii)\n",
    "        #print((v - v_prev) / flag)\n",
    "        v_prev = v\n",
    "    return Y_hat, mu_hat, U, s, Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cb53bd782aaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memsvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "X=pd.DataFrame(emsvd(df3,k=2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(emsvd(df3,k=2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(X)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "La méthode du coude nous donne qu'il faut prendre k=4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(X)\n",
    "X['Cluster']=kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(X),hue='Cluster')"
   ]
  },
  {
   "source": [
    "Méthode de factorisation matricielle\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " On décide maintenant d'utiliser la méthode de factorisation matricielle afin de résoudre notre problème initial, cette méthode non supervisée se base sur des latent features dont on cherchera à déterminer le nombre optimal, cette méthode nous permet de prédire l'ensemble des notes que les utilisateurs donneraient aux films."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set(dat=ratings,col='UserID',alpha=0.8):\n",
    "    data=dat.copy()\n",
    "    users=data[col].unique()\n",
    "    random=[rd.uniform(0,1) for k in range (0,data.shape[0])]\n",
    "    data['rd']=random\n",
    "    training=data[data['rd']<alpha]\n",
    "    for i in users:\n",
    "        index=data[col]==i\n",
    "        training_user=data[index].reset_index()\n",
    "        training_user=training_user.loc[0]\n",
    "        training_user=training_user.to_frame().T\n",
    "        if training.merge(training_user,how='inner').shape[0]==0:\n",
    "            training=training.merge(training_user)\n",
    "\n",
    "        #On garde au moins une note pour chaque utilisateur \n",
    "    training=pd.concat([training,data[data['rd']<alpha]],join='outer',sort=False)\n",
    "    test=data[data['rd']>alpha]\n",
    "    del training['rd']\n",
    "    del test['rd']\n",
    "    return(training,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On split la base de données afin de vérifier nos résultats\n",
    "training,test = data_set(ratings,'UserID',0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_training=training.pivot(index='UserID',columns='MovieID',values='Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_test=test.pivot(index='UserID',columns='MovieID',values='Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_training_standard=standard(notes_training.T,notes_training.T)\n",
    "notes_test_standard=standard(notes_test.T,notes_training.T)"
   ]
  },
  {
   "source": [
    "On cherche à optimiser la valeur de nombre de latent feature. On regarde la variation d'écart pour un jeu de donnée selon le nombre choisi."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "erreur=[]\n",
    "x=[]\n",
    "for i in range (1,15):\n",
    "    note_estimate=(pd.DataFrame(emsvd(notes_training_standard.astype(float),k=i)[0]))\n",
    "    e=(note_estimate-notes_test_standard)**2\n",
    "    erreur.append(e.mean().mean())\n",
    "    x.append(i)\n",
    "plt.plot(x,erreur)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(erreur))"
   ]
  },
  {
   "source": [
    "On choisit un modèle à 1 latent feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=1\n",
    "notes_test_standard['Cluster']=X['Cluster']\n",
    "erreur=0\n",
    "for i in range(0,4):\n",
    "    notes_cluster=notes_training_standard[notes_test_standard['Cluster']==i]\n",
    "    #del notes_cluster['UserID']\n",
    "    test_cluster=notes_test_standard[notes_test_standard['Cluster']==i]\n",
    "    #del test_cluster['UserID']\n",
    "    del test_cluster['Cluster']\n",
    "    notes_cluster_estimate=pd.DataFrame(emsvd(notes_cluster.astype(float),k=n_features)[0])\n",
    "    e=(notes_cluster_estimate-notes_cluster)**2\n",
    "    erreur+=e.mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(erreur/4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}