{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2/ Une approche collaborative\n",
    "\n",
    "On regarde ensuite les notes obtenues selon les clusters de films pour chaque individu. On espère trouver des types d'individus : on aime l'action, et/ou on aime la comedie, ... \n",
    "  \n",
    "On veut un dataframe avec, pour chaque utilisateur, la note moyenne qu'il a donné aux films qui appartiennent à chaque cluster . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le genre n'est pas explicatif pour la population globale, mais c'est peut être le cas pour certains d'individus.  \n",
    "On crée un dataframe pref qui donne pour chaque utilisateur la note moyenne selon la catégorie. On normalise les ratings des utilisateurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On normalise les notes des utilisateurs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=ratings.pivot(index='UserID',columns='MovieID',values='Rating').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard(data_ref,data):\n",
    "    #On retranche d'abord la moyenne \n",
    "    df=data.sub(data_ref.mean())\n",
    "    std=df.std(axis = 0, skipna = True) \n",
    "    std[std==0]=1\n",
    "    df=df/std\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=standard(df0,df0)\n",
    "#df1=(df0 - df0.min()) / (df0.max() - df0.min())\n",
    "df1['MovieID']=pd.unique(ratings.sort_values(by='MovieID')['MovieID'])\n",
    "del df1.index.name\n",
    "df2=df.merge(df1,on='MovieID',how='inner')\n",
    "index_movies=df2['MovieID']\n",
    "index_movies=index_movies.to_frame()\n",
    "df1=df1.merge(index_movies,on='MovieID',how='inner')\n",
    "del df1['MovieID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.DataFrame()\n",
    "for i in Cluster : \n",
    "    mask=df2[i]==1\n",
    "    df3[i]=df1[mask].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_pref=df3.copy()\n",
    "#on remplit les valeurs manquantes avec \n",
    "indiv_pref=indiv_pref.T\n",
    "indiv_pref=indiv_pref.fillna(indiv_pref.mean())\n",
    "indiv_pref=indiv_pref.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Corrélations entre les notes moyenne par clusters')\n",
    "sns.heatmap(indiv_pref.corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Préférences des individus pour les différents clusters')\n",
    "sns.heatmap(indiv_pref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certaines catégories font plus l'unanimité que d'autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un algorithme de clustering : la méthode k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = indiv_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(X)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trouve k=3 par la méthode du coude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = indiv_pref\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trace selon les différents clusters\n",
    "fig = px.scatter_3d(indiv_pref, x='Cluster Action', y='Cluster Drama',z='Cluster Animation',color=kmeans)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(indiv_pref, x='Cluster Comedy', y='Cluster War',z='Cluster Thriller',color=kmeans)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On represente les clusters d'individus dans le plan d'une ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doit avoir des colonnes pour ACP \n",
    "std_scale=preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = std_scale.transform(X)\n",
    "\n",
    "#Calcul composantes principales \n",
    "pca=decomposition.PCA(n_comp)\n",
    "pca.fit(X_scaled)\n",
    "                \n",
    "#display_scree_plot(pca)\n",
    "coord=pca.fit_transform(X)\n",
    "pcs=pca.components_\n",
    "#display_circles(pcs,n_comp,pca,[(0,1),(2,3),(4,5)])\n",
    "\n",
    "#print(pca.explained_variance_ratio_)\n",
    "#print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph=pd.DataFrame(coord,columns=['x','y','z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_graph, x='x', y='y',z='z',color=kmeans)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche quelques statistiques pour savoir la pertinence des clusters pour déterminer une influence sur le rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=indiv_pref.copy()\n",
    "df4['Cluster']=kmeansOn utilise ensuite sns.pairplot afin de visualiser selon les différents clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise ensuite sns.pairplot afin de visualiser selon les différents clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df4,hue='Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Cluster:\n",
    "    sns.boxplot(y=i,x='Cluster',data=df4)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant tester un algorithme SVD pour obtenir la matrice complète des notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def emsvd(Y, k=None, tol=1E-3, maxiter=150):\n",
    "    \"\"\"\n",
    "    Approximate SVD on data with missing values via expectation-maximization\n",
    "\n",
    "    Inputs:\n",
    "    -----------\n",
    "    Y:          (nobs, ndim) data matrix, missing values denoted by NaN/Inf\n",
    "    k:          number of singular values/vectors to find (default: k=ndim)\n",
    "    tol:        convergence tolerance on change in trace norm\n",
    "    maxiter:    maximum number of EM steps to perform (default: no limit)\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    Y_hat:      (nobs, ndim) reconstructed data matrix\n",
    "    mu_hat:     (ndim,) estimated column means for reconstructed data\n",
    "    U, s, Vt:   singular values and vectors (see np.linalg.svd and \n",
    "                scipy.sparse.linalg.svds for details)\n",
    "    \"\"\"\n",
    "\n",
    "    if k is None:\n",
    "        svdmethod = partial(np.linalg.svd, full_matrices=False)\n",
    "    else:\n",
    "        svdmethod = partial(svds, k=k)\n",
    "    if maxiter is None:\n",
    "        maxiter = np.inf\n",
    "\n",
    "    # initialize the missing values to their respective column means\n",
    "    mu_hat = np.nanmean(Y, axis=0, keepdims=1)\n",
    "    valid = np.isfinite(Y)\n",
    "    Y_hat = np.where(valid, Y, mu_hat)\n",
    "\n",
    "    halt = False\n",
    "    ii = 1\n",
    "    v_prev = 0\n",
    "\n",
    "    while not halt:\n",
    "        \n",
    "    # SVD on filled-in data\n",
    "        U, s, Vt = svdmethod(Y_hat - mu_hat)\n",
    "\n",
    "        # impute missing values\n",
    "        Y_hat[~valid] = (U.dot(np.diag(s)).dot(Vt) + mu_hat)[~valid]\n",
    "\n",
    "        # update bias parameter\n",
    "        mu_hat = Y_hat.mean(axis=0, keepdims=1)\n",
    "\n",
    "        # test convergence using relative change in trace norm\n",
    "        v = s.sum()\n",
    "        if v_prev==0:\n",
    "            flag=1\n",
    "        else:\n",
    "            flag=v_prev\n",
    "        if ii >= maxiter or ((v - v_prev) / flag) < tol:\n",
    "            halt = True\n",
    "        ii += 1\n",
    "        #print(ii)\n",
    "        #print((v - v_prev) / flag)\n",
    "        v_prev = v\n",
    "    return Y_hat, mu_hat, U, s, Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(emsvd(df3,k=2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(X)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode du coude nous donne qu'il faut prendre k=4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(X)\n",
    "X['Cluster']=kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(X),hue='Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut identifier les clusters à des individus qui aiment certains catégories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de factorisation matricielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " On décide maintenant d'utiliser la méthode de factorisation matricielle afin de résoudre notre problème initial, cette méthode non supervisée se base sur des latent features dont on cherchera à déterminer le nombre optimal, cette méthode nous permet de prédire l'ensemble des notes que les utilisateurs donneraient aux films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set(dat=ratings,col='UserID',alpha=0.8):\n",
    "    data=dat.copy()\n",
    "    users=data[col].unique()\n",
    "    random=[rd.uniform(0,1) for k in range (0,data.shape[0])]\n",
    "    data['rd']=random\n",
    "    training=data[data['rd']<alpha]\n",
    "    for i in users:\n",
    "        index=data[col]==i\n",
    "        training_user=data[index].reset_index()\n",
    "        training_user=training_user.loc[0]\n",
    "        training_user=training_user.to_frame().T\n",
    "        if training.merge(training_user,how='inner').shape[0]==0:\n",
    "            training=training.merge(training_user)\n",
    "\n",
    "        #On garde au moins une note pour chaque utilisateur \n",
    "    training=pd.concat([training,data[data['rd']<alpha]],join='outer',sort=False)\n",
    "    test=data[data['rd']>alpha]\n",
    "    del training['rd']\n",
    "    del test['rd']\n",
    "    return(training,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On split la base de données afin de vérifier nos résultats\n",
    "training,test = data_set(ratings,'UserID',0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_training=training.pivot(index='UserID',columns='MovieID',values='Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_test=test.pivot(index='UserID',columns='MovieID',values='Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_training_standard=standard(notes_training.T,notes_training.T)\n",
    "notes_test_standard=standard(notes_test.T,notes_training.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche à optimiser la valeur de nombre de latent feature. On regarde la variation d'écart pour un jeu de donnée selon le nombre choisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erreur=[]\n",
    "x=[]\n",
    "for i in range (1,10):\n",
    "    note_estimate=(pd.DataFrame(emsvd(notes_training_standard.astype(float),k=i)[0]))\n",
    "    e=(note_estimate-notes_test_standard)**2\n",
    "    erreur.append(e.mean().mean())\n",
    "    x.append(i)\n",
    "plt.plot(x,erreur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(erreur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit un modèle à 1 latent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=1\n",
    "notes_test_standard['Cluster']=X['Cluster']\n",
    "erreur=0\n",
    "for i in range(0,4):\n",
    "    notes_cluster=notes_training_standard[notes_test_standard['Cluster']==i]\n",
    "    #del notes_cluster['UserID']\n",
    "    test_cluster=notes_test_standard[notes_test_standard['Cluster']==i]\n",
    "    #del test_cluster['UserID']\n",
    "    del test_cluster['Cluster']\n",
    "    notes_cluster_estimate=pd.DataFrame(emsvd(notes_cluster.astype(float),k=n_features)[0])\n",
    "    e=(notes_cluster_estimate-test_cluster)**2\n",
    "    erreur+=e.mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(erreur/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que l'erreur quadratique moyenne ne diminue pas lorsqu'on divise la matrice en cluster d'individu aux gouts similaires, on garde une erreur moyenne importante, mais le clustering d'individus permet une légère amélioration des résultats obtenus. Le temps de calcul nécessaire est également largement diminué."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
